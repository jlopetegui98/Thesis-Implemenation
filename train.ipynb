{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data for experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps for sentences preprocessing:\n",
    "1. Get content words for each sentence in the corpus\n",
    "2. Create vocabulary for bag of words from those words\n",
    "3. Remove sentences without content words\n",
    "4. Get embeddings of this sentences\n",
    "5. Save embeddings, content words of sentences and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from content_model_pytorch.config import ModelConfig\n",
    "from content_model_pytorch.data_loader import SentencesDataset\n",
    "from content_model_pytorch.model import ContentModel\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets Path\n",
    "DATASETS_PATH = \"./datasets/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"formality-score\"\n",
    "\n",
    "# load content words for dataset\n",
    "with open(f\"{DATASETS_PATH}formality-score/sentences_content_words/train.txt\", 'r+') as fd:\n",
    "    sentences_content_words = fd.readlines()\n",
    "\n",
    "# count number of sentences in dataset without content words and build\n",
    "# accumulate table\n",
    "cont = 0\n",
    "acc_table = []\n",
    "for sentence in sentences_content_words:\n",
    "    if sentence == '\\n':\n",
    "        cont += 1\n",
    "    acc_table.append(cont)\n",
    "\n",
    "# remove sentences without content words\n",
    "sentences_content_words = [(sentence,idx) for idx,sentence in enumerate(sentences_content_words) if sentence != '\\n']\n",
    "# len(sentences_content_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"formality-score\"\n",
    "\n",
    "# load content words for dataset\n",
    "with open(f\"{DATASETS_PATH}formality-score/sentences_content_words/test.txt\", 'r+') as fd:\n",
    "    sentences_content_words_val = fd.readlines()\n",
    "# print(len(sentences_content_words_val))\n",
    "# count number of sentences in dataset without content words and build\n",
    "# accumulate table for validation\n",
    "cont = 0\n",
    "acc_table_val = []\n",
    "for sentence in sentences_content_words_val:\n",
    "    if sentence == '\\n':\n",
    "        cont += 1\n",
    "    acc_table_val.append(cont)\n",
    "\n",
    "# remove sentences without content words\n",
    "sentences_content_words_val = [(sentence,idx) for idx,sentence in enumerate(sentences_content_words_val) if sentence != '\\n']\n",
    "# len(sentences_content_words_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = f\"{DATASETS_PATH}{dataset}/embeddings/\"\n",
    "# create data_loader instance\n",
    "dataset = SentencesDataset(embeddings_path,\n",
    "        sentences_content_words,\n",
    "        mode=\"train\",batches_size = 100)\n",
    "# create data_loader instance for validation\n",
    "dataset_val = SentencesDataset(embeddings_path,\n",
    "        sentences_content_words_val,\n",
    "        mode=\"test\",word2indx=dataset.vocab.word2indx,batches_size = 100)\n",
    "# Load Model\n",
    "config = ModelConfig(len(dataset.vocab.word2indx), embedding_size = 100, number_of_layers = 10)\n",
    "data_loader = DataLoader(dataset,100)\n",
    "data_loader_val = DataLoader(dataset_val,100)\n",
    "\n",
    "# create model instance\n",
    "model = ContentModel(config)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training started!\")\n",
    "loss_list = []\n",
    "loss_list_val = []\n",
    "for epoch in trange(20, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_loss_val = 0\n",
    "    for i, (sentences_embeddings, sentences_bow) in enumerate(tqdm(data_loader, desc=\"Iteration\")):\n",
    "        # print(sentences_embeddings == None)\n",
    "        # print(sentences_bow == None)\n",
    "        # print(len(sentences_embeddings))\n",
    "        sentences_embeddings.to(device)\n",
    "        sentences_bow.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        # print(len(sentences_embeddings), sentences_embeddings[0].shape, sentences_bow[0].shape)\n",
    "\n",
    "        loss = model(sentences_embeddings,sentences_bow)\n",
    "        # calculate loss\n",
    "        # loss = nn.BCELoss()(output, sentences_bow)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    # loss for validation\n",
    "    model.eval()\n",
    "    for i, (sentences_embeddings, sentences_bow) in enumerate(tqdm(data_loader_val, desc=\"Iteration\")):\n",
    "        # print(sentences_embeddings == None)\n",
    "        # print(sentences_bow == None)\n",
    "        sentences_embeddings.to(device)\n",
    "        sentences_bow.to(device)\n",
    "        # forward pass\n",
    "        with torch.no_grad():\n",
    "            loss = model(sentences_embeddings,sentences_bow)\n",
    "            total_loss_val += loss.item()\n",
    "    loss_list.append(total_loss/len(data_loader))\n",
    "    loss_list_val.append(total_loss_val/len(data_loader_val))\n",
    "    print(f\"Epoch: {epoch}, Loss: {total_loss/len(data_loader)}, Loss_val: {total_loss_val/len(data_loader_val)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_list)\n",
    "# plt.plot(loss_list_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b54bf50e215961265705773e556e63326e1cc63981e516147023b780df3eb29f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
